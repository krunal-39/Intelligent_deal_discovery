{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfb8491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Environment, imports, paths\n",
    "import os, time, math\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027686cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Environment token + quick GPU info (you provided HF_TOKEN in .env)\n",
    "HF_TOKEN = os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "assert HF_TOKEN, \"‚ùå Missing Hugging Face token! Please check your .env file.\"\n",
    "\n",
    "print(\"‚úÖ Env ready. CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}:\", torch.cuda.get_device_name(i), \n",
    "              \"| Memory total (GB):\", round(torch.cuda.get_device_properties(i).total_memory/1e9,1))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPUs detected - training will be very slow or fail.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774f0198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHS (use your values)\n",
    "DATA_DIR = Path(\"/data/home/anjeshnarwal/LLM_price_predictor/data/finetune\")\n",
    "TRAIN_PATH = DATA_DIR / \"train.jsonl\"\n",
    "VAL_PATH   = DATA_DIR / \"val.jsonl\"\n",
    "OUTPUT_DIR = Path(\"../src/models/llama31_8b_qlora_full\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Quick reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0f52e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load dataset (expects train.jsonl / validation.jsonl in DATA_DIR)\n",
    "from datasets import load_dataset\n",
    "\n",
    "if \"raw_datasets\" in globals():\n",
    "    print(\"‚úÖ Reusing existing raw_datasets\")\n",
    "else:\n",
    "    assert TRAIN_PATH.exists() and VAL_PATH.exists(), f\"Train/Val not found at {TRAIN_PATH} / {VAL_PATH}\"\n",
    "    print(\"üîÅ Loading JSONL dataset from disk (this may take a minute)...\")\n",
    "    raw_datasets = load_dataset(\"json\", data_files={\"train\": str(TRAIN_PATH), \"validation\": str(VAL_PATH)})\n",
    "    \n",
    "print(\"DatasetDict:\", raw_datasets)\n",
    "print(\"Train rows:\", len(raw_datasets[\"train\"]), \"Validation rows:\", len(raw_datasets[\"validation\"]))\n",
    "# Quick sample check\n",
    "for i, ex in enumerate(raw_datasets[\"train\"].select(range(0, 3))):\n",
    "    print(f\"\\nSAMPLE {i} prompt (trunc):\", ex[\"prompt\"][:200].replace(\"\\n\",\" \"), \" -> response:\", ex[\"response\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9255c2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Tokenizer and optional pre-tokenization (recommended)\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3.1-8B\"  # or your chosen base\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=False, trust_remote_code=True, use_auth_token=HF_TOKEN)\n",
    "\n",
    "# ensure pad token\n",
    "if tokenizer.pad_token is None:\n",
    "    if tokenizer.eos_token is not None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    else:\n",
    "        tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "print(\"Tokenizer loaded. vocab size:\", len(tokenizer))\n",
    "\n",
    "# ---- Pre-tokenize (batched) ----\n",
    "DO_PRETOKENIZE = True  # set False to skip and rely on collator on-the-fly tokenization\n",
    "\n",
    "if DO_PRETOKENIZE:\n",
    "    def tokenize_batch(examples):\n",
    "        # tokenizes prompt+response together so collator can only build labels easily\n",
    "        texts = [p + \" \" + str(r) for p, r in zip(examples[\"prompt\"], examples[\"response\"])]\n",
    "        out = tokenizer(texts, truncation=True, padding=False, max_length=512, add_special_tokens=False)\n",
    "        return {\"input_ids\": out[\"input_ids\"]}\n",
    "\n",
    "    # run with multiple processes to speed up\n",
    "    print(\"üîÅ Pre-tokenizing train split (num_proc=8)...\")\n",
    "    raw_datasets[\"train\"] = raw_datasets[\"train\"].map(tokenize_batch, batched=True, batch_size=1024, remove_columns=raw_datasets[\"train\"].column_names, num_proc=8)\n",
    "    print(\"üîÅ Pre-tokenizing validation split (num_proc=4)...\")\n",
    "    raw_datasets[\"validation\"] = raw_datasets[\"validation\"].map(tokenize_batch, batched=True, batch_size=1024, remove_columns=raw_datasets[\"validation\"].column_names, num_proc=4)\n",
    "    # rename columns to keep prompt/response not required ‚Äî we'll use input_ids and reconstruct response tokens in collator\n",
    "    print(\"‚úÖ Pre-tokenization complete.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping pre-tokenization; collator will tokenise on the fly.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0228354c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Cell 4: Data collator (pre-tokenized mode only)\n",
    "# ============================================\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "# --- Add project root to sys.path (for both .py & Jupyter modes) ---\n",
    "try:\n",
    "    # __file__ is defined when run as a .py script\n",
    "    current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    # Fallback for interactive or Jupyter mode\n",
    "    current_dir = os.getcwd()\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    print(f\"üß© Added project root to sys.path: {project_root}\")\n",
    "\n",
    "# --- Import your collator ---\n",
    "from src.collator import DataCollatorForPricePrediction\n",
    "\n",
    "# --- Instantiate collator ---\n",
    "collator = DataCollatorForPricePrediction(tokenizer=tokenizer, max_length=512)\n",
    "\n",
    "# ‚úÖ Tokenizer sanity check\n",
    "print(\"üß† Collator initialized successfully:\")\n",
    "print(f\"   ‚Ä¢ Tokenizer pad_token: '{tokenizer.pad_token}' (ID: {tokenizer.pad_token_id})\")\n",
    "print(f\"   ‚Ä¢ Collator max_length: {collator.max_length}\")\n",
    "\n",
    "# ‚úÖ Simple smoke test using actual pre-tokenized data\n",
    "if len(raw_datasets[\"train\"]) >= 4:\n",
    "    sample = raw_datasets[\"train\"].select(range(4))\n",
    "    batch = collator(sample)\n",
    "    print(\"‚úÖ Collator test successful.\")\n",
    "    for k, v in batch.items():\n",
    "        print(f\"{k}: {tuple(v.shape)} dtype={v.dtype}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Not enough samples for collator quick test.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7f81a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Cell 5: Load model in 4-bit and prepare for k-bit training\n",
    "# ============================================\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "\n",
    "# --- Memory-efficient quantization config ---\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16   # ‚úÖ FP16 compute for RTX A6000 Tensor Cores\n",
    ")\n",
    "\n",
    "print(\"üîß Loading model in 4-bit (this can take some time)...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"auto\",                    # ‚úÖ auto-balance layers across both GPUs\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    token=HF_TOKEN,                       # ‚úÖ replaces deprecated use_auth_token\n",
    "    torch_dtype=torch.bfloat16            # ‚úÖ safe, stable load precision\n",
    ")\n",
    "print(\"‚úÖ Model loaded in 4-bit.\")\n",
    "\n",
    "# --- Resize embeddings if tokenizer was extended ---\n",
    "if model.get_input_embeddings().weight.shape[0] < len(tokenizer):\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    print(\"Resized model embeddings.\")\n",
    "\n",
    "# --- Prepare for QLoRA fine-tuning ---\n",
    "print(\"‚öôÔ∏è Preparing model for k-bit training with gradient checkpointing...\")\n",
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "\n",
    "# --- Convert leftover FP32 tensors to FP16 to reclaim VRAM ---\n",
    "for p in model.parameters():\n",
    "    if p.dtype == torch.float32:\n",
    "        p.data = p.data.to(torch.float16)\n",
    "print(\"‚úÖ Model prepared and memory optimized (FP32 ‚Üí FP16 where safe).\")\n",
    "\n",
    "# --- Define LoRA target modules ---\n",
    "target_modules = ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\n",
    "\n",
    "# --- LoRA configuration ---\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=target_modules,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# --- Apply PEFT ---\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# --- Print summary of trainable parameters ---\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable params: {trainable:,} / {total:,} ({100*trainable/total:.6f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6559de9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Cell 6: Optimized TrainingArguments and Trainer initialization (plain Python)\n",
    "# ============================================\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Hardware-informed params (2x A6000)\n",
    "PER_DEVICE_TRAIN_BATCH = 12           # adjust to 12 if VRAM allows; 8 if OOM\n",
    "GRADIENT_ACCUM_STEPS = 2              # effective batch = PER_DEVICE * ACCUM * GPUs\n",
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "# Checkpoint/eval frequency\n",
    "EVAL_STEPS = 30000                    # slightly more frequent eval (optional)\n",
    "SAVE_STEPS = 30000\n",
    "LOGGING_STEPS = 1000\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(OUTPUT_DIR),\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH,\n",
    "    per_device_eval_batch_size=PER_DEVICE_TRAIN_BATCH,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUM_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    fp16=True,                        # ‚úÖ use mixed precision (fast on A6000)\n",
    "    bf16=False,                       # ‚úÖ explicitly disable bf16 training\n",
    "    optim=\"adamw_bnb_8bit\",           # ‚úÖ bitsandbytes optimizer for QLoRA\n",
    "    warmup_ratio=0.03,\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=3,\n",
    "    dataloader_num_workers=8,         # ‚úÖ parallel dataloading\n",
    "    group_by_length=True,\n",
    "    gradient_checkpointing=True,      # ‚úÖ memory saving\n",
    "    remove_unused_columns=False,\n",
    "    torch_compile=False,              # ‚úÖ avoid compilation overhead\n",
    "    report_to=\"none\"                  # ‚úÖ no W&B / HF Hub logging\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=raw_datasets[\"train\"],\n",
    "    eval_dataset=raw_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator\n",
    ")\n",
    "\n",
    "# --- Expected steps and sanity checks ---\n",
    "num_train = len(trainer.train_dataset)\n",
    "gpus = torch.cuda.device_count() or 1\n",
    "steps_per_epoch = math.ceil(num_train / (PER_DEVICE_TRAIN_BATCH * gpus * GRADIENT_ACCUM_STEPS))\n",
    "total_steps = steps_per_epoch * NUM_EPOCHS\n",
    "print(f\"Num train examples: {num_train:,}; GPUs: {gpus}\")\n",
    "print(f\"Steps/epoch ‚âà {steps_per_epoch:,}; Total steps ‚âà {total_steps:,}\")\n",
    "\n",
    "assert num_train > 1000, \"Train dataset suspiciously small.\"\n",
    "assert trainable > 0, \"Trainable params = 0. Check LoRA target_modules or get_peft_model.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75b53a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 7: Measure dataloader throughput (pre-tokenized smoke test)\n",
    "# import time\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# # small slice of dataset for timing\n",
    "# sample_ds = raw_datasets[\"train\"].select(range(0, 512))\n",
    "\n",
    "# # plain dataloader that just pads via collator (pretokenized)\n",
    "# dl = DataLoader(sample_ds, batch_size=training_args.per_device_train_batch_size,\n",
    "#                 shuffle=False, collate_fn=collator, num_workers=4)\n",
    "\n",
    "# n_batches = 10\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# t0 = time.time()\n",
    "# for i, batch in enumerate(dl):\n",
    "#     if i >= n_batches:\n",
    "#         break\n",
    "#     batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n",
    "#     with torch.no_grad():\n",
    "#         outputs = trainer.model(**batch)\n",
    "# t1 = time.time()\n",
    "\n",
    "# avg_batch_time = (t1 - t0) / n_batches\n",
    "# it_per_sec = 1.0 / avg_batch_time\n",
    "# print(f\"‚ö° Avg batch time (data+forward): {avg_batch_time:.3f}s  =>  {it_per_sec:.2f} it/s\")\n",
    "\n",
    "# effective_step_time = avg_batch_time * training_args.gradient_accumulation_steps\n",
    "# print(f\"‚è±Ô∏è  Est. optimizer step time (grad_accum={training_args.gradient_accumulation_steps}): \"\n",
    "#       f\"{effective_step_time:.3f}s  =>  {1.0/effective_step_time:.2f} steps/s\")\n",
    "# print(\"Use these numbers to estimate total ETA: total_steps * step_time\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760b1fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Cell 8: Start training (Plain Python version)\n",
    "# ============================================\n",
    "import time, sys, os\n",
    "\n",
    "print(\"üöÄ Starting full QLoRA fine-tuning (plain Python, no accelerate)...\")\n",
    "\n",
    "# --- Optional resume support from CLI ---\n",
    "resume_from_checkpoint = None\n",
    "if \"--resume_from_checkpoint\" in sys.argv:\n",
    "    idx = sys.argv.index(\"--resume_from_checkpoint\")\n",
    "    if idx + 1 < len(sys.argv):\n",
    "        resume_from_checkpoint = sys.argv[idx + 1]\n",
    "        if os.path.exists(resume_from_checkpoint):\n",
    "            print(f\"üîÅ Resuming training from checkpoint: {resume_from_checkpoint}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Checkpoint path not found: {resume_from_checkpoint}. Starting fresh.\")\n",
    "\n",
    "# --- Training start ---\n",
    "start_time = time.time()\n",
    "try:\n",
    "    train_result = trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "    trainer.save_model(str(OUTPUT_DIR))      # saves adapters and config\n",
    "    trainer.save_state()\n",
    "    print(f\"‚úÖ Model + training state saved to: {OUTPUT_DIR}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Training crashed due to error: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- Duration report ---\n",
    "elapsed = time.time() - start_time\n",
    "hours = int(elapsed // 3600)\n",
    "minutes = int((elapsed % 3600) // 60)\n",
    "seconds = int(elapsed % 60)\n",
    "print(f\"‚úÖ Training completed in {hours}h {minutes}m {seconds}s\")\n",
    "print(\"Train result summary:\", train_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4600fa2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
